{
 cells [
  {
   cell_type markdown,
   metadata {},
   source [
    # CSC321 Tutorial 5 Backpropagationn,
    n,
    We've seen in lecture that a linear classifier is bound to produce errors ifn,
    our data is not linearly separable. We can avoid this issue by using a moren,
    powerful classifier like a multi-layer perceptron (aka a neural networkn,
    with fully-connected layers).n,
    n,
    In this tutorial, we examine a classification problem for which the datan,
    is not linearly separable. We will implement a 2-layer neural network andn,
    train it using gradient descent, computing gradients using then,
    backpropagation algorithm.
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    import matplotlibn,
    import numpy as npn,
    import matplotlib.pyplot as pltn,
    from scipy.special import expit as sigmoidn,
    import mathn,
    %matplotlib inline
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Datan,
    n,
    We will generate a toy data set, similar to the one that you saw in n,
    httpplayground.tensorflow.org
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    np.random.seed(0)n,
    n,
    def make_dataset(num_points)n,
        radius = 5n,
        data = []n,
        labels = []n,
        # Generate positive examples (labeled 1).n,
        for i in range(num_points  2)n,
            r = np.random.uniform(0, radius0.5)n,
            angle = np.random.uniform(0, 2math.pi)n,
            x = r  math.sin(angle)n,
            y = r  math.cos(angle)n,
            data.append([x, y])n,
            labels.append(1)n,
            n,
        # Generate negative examples (labeled 0).n,
        for i in range(num_points  2)n,
            r = np.random.uniform(radius0.7, radius)n,
            angle = np.random.uniform(0, 2math.pi)n,
            x = r  math.sin(angle)n,
            y = r  math.cos(angle)n,
            data.append([x, y])n,
            labels.append(0)n,
            n,
        data = np.asarray(data)n,
        labels = np.asarray(labels)n,
        return data, labelsn,
        n,
    num_data = 500n,
    data, labels = make_dataset(num_data)n,
    n,
    # Note red indicates a label of 1, blue indicates a label of 0n,
    plt.scatter(data[num_data2, 0], data[num_data2, 1], color='red') n,
    plt.scatter(data[num_data2, 0], data[num_data2, 1], color='blue')   
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Neural network definitionn,
    n,
    We will try to classify this data by training a neural network. As a reminder, our goal is to take as input a two dimensional vector $mathbf{x} = [x_1, x_2]^T$ and output $text{Pr}(t = 1  mathbf{x})$, where $t$ is the label of the datapoint $mathbf{x}$. n,
    n,
    We will use a neural network with one hidden layer which has three hidden units. The equations describing our neural network are belown,
    n,
    $$mathbf{g} = mathbf{U} mathbf{x} + mathbf{b}$$n,
    $$mathbf{h} = tanh(mathbf{g})$$n,
    $$z = mathbf{W} mathbf{h} + c$$n,
    $$y = sigma(z)$$n,
    n,
    In the equations above, $mathbf{U} = begin{pmatrix} u_{11} & u_{12}  u_{21} & u_{22}   u_{31} & u_{32} end{pmatrix} in mathbb{R}^{3 times 2}, mathbf{b} = begin{pmatrix} b_1   b_2  b_3 end{pmatrix} in mathbb{R}^3, mathbf{W} = begin{pmatrix} w_{1} & w_{2} & w_{3} end{pmatrix} in mathbb{R}^{1 times 3}, c in mathbb{R}$ are the parameters of our neural network which we must learn. Notice we are writing $mathbf{W}$ as a matrix with one row.n,
    n,
    n,
    ## Vectorizing the neural networkn,
    n,
    We want our neural network to produce predictions for multiple points efficiently. We can do so by vectorizing over training examples. Let  $mathbf{X} = begin{pmatrix} x_{11} & x_{12}  vdots   & vdots    x_{N1} & x_{N2}n,
    end{pmatrix}$ be a matrix containing $N$ datapoints in separate rows. Then we can vectorize by usingn,
    n,
    $$mathbf{G} = mathbf{X}mathbf{U}^T + mathbf{1}mathbf{b}^T$$n,
    $$mathbf{H} = tanh(mathbf{G})$$n,
    $$mathbf{z} =  mathbf{H}mathbf{W}^T + mathbf{1}c$$n,
    $$mathbf{y} = sigma(mathbf{z})$$n,
    n,
    $mathbf{G}$, for example, will store each of the three hidden unit values for each datapoint in each corresponding row.n,
    n,
    We can rewrite in scalar form asn,
    $$g_{ij} = u_{j1} x_{i1} + u_{j2} x_{i2} + b_j$$n,
    $$h_{ij} = tanh(g_{ij})$$n,
    $$z_{i} = w_1 h_{i1} + w_2 h_{i2} + w_{3} h_{i3} + c$$n,
    $$y_i = sigma(z_i)$$n,
    Here, $i$ indexes data points and $j$ indexes hidden units, so $i in {1, dots, N}$ and $j in {1, 2, 3}$.
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    # First, initialize our neural network parameters.n,
    params = {}n,
    params['U'] = np.random.randn(3, 2)n,
    params['b'] = np.zeros(3)n,
    params['W'] = np.random.randn(3)n,
    params['c'] = 0n,
    n,
    # Notice we make use of numpy's broadcasting when adding the bias b.n,
    def forward(X, params)    n,
        G = np.dot(X, params['U'].T)  + params['b']n,
        H = np.tanh(G)n,
        z = np.dot(H, params['W'].T) + params['c']n,
        y = sigmoid(z)n,
        n,
        return y
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Visualize the network's predictionsn,
    n,
    Let's visualize the predictions of our untrained network. As we can see, the network does not succeed at classifying the points without training
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    num_points = 200n,
    x1s = np.linspace(-6.0, 6.0, num_points)n,
    x2s = np.linspace(-6.0, 6.0, num_points)n,
    n,
    points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])n,
    Y = forward(points, params).reshape(num_points, num_points)n,
    X1, X2 = np.meshgrid(x1s, x2s)n,
    n,
    plt.pcolormesh(X1, X2, Y, cmap=plt.cm.get_cmap('YlGn'))n,
    plt.colorbar()n,
    plt.scatter(data[num_data2, 0], data[num_data2, 1], color='red') n,
    plt.scatter(data[num_data2, 0], data[num_data2, 1], color='blue') 
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Loss functionn,
    n,
    We will use the same cross entropy loss function as in logistic regression. This loss function isn,
    n,
    $$mathcal{L}_{CE}(y, t) = -t log(y) - (1 - t)log(1 - y)$$n,
    n,
    Here $y = Pr(t = 1mathbf{x})$ and $t$ is the true label.n,
    n,
    Remember that computing the derivative of this loss function $frac{d L}{dy}$ can become numerically unstable. Instead, we combine the logistic function and the cross entropy loss into a single function called logistic cross-entropyn,
    n,
    $$mathcal{L}_{LCE}(z, t) = t log(1 + exp(-z)) + (1 -t) log(1 + exp(z))$$n,
    n,
    See Lecture 4 Notes for review on this. n,
    n,
    Our cost function is the sum over multiple examples of the loss function, normalized by the number of examplesn,
    n,
    $$mathcal{E}(mathbf{z}, mathbf{t}) = frac{1}{N} left[sum_{i=1}^N mathcal{L}(z_i, t_i)right]$$n,
    n,
    ## Derive backpropagation equationsn,
    n,
    We now derive the backpropagation equations in scalar form and then vectorize on the board.n,
    n,
    n,
    ## Implement backpropagation equations
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    def backprop(X, t, params)n,
        N = X.shape[0]n,
        n,
        # Perform forwards computation.n,
        G = np.dot(X, params['U'].T)  + params['b']n,
        H = np.tanh(G)n,
        z = np.dot(H, params['W'].T) + params['c']n,
        y = sigmoid(z)n,
        loss = (1.N)  np.sum(-t  np.log(y) - (1 - t)  np.log(1 - y))n,
        n,
        # Perform backwards computation.n,
        E_bar = 1n,
        z_bar = (1.N)  (y - t)n,
        W_bar = np.dot(H.T, z_bar)n,
        c_bar = np.dot(z_bar, np.ones(N))n,
        H_bar = np.outer(z_bar, params['W'].T)n,
        G_bar = H_bar  (1 - np.tanh(G)2)n,
        U_bar = np.dot(G_bar.T, X)n,
        b_bar = np.dot(G_bar.T, np.ones(N))n,
        n,
        # Wrap our gradients in a dictionary.n,
        grads = {}n,
        grads['U'] = U_barn,
        grads['b'] = b_barn,
        grads['W'] = W_barn,
        grads['c'] = c_barn,
        n,
        return grads, loss
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Training the networkn,
    n,
    We can train our network parameters using gradient descent once we have computed derivatives using the backpropagation algorithm. Recall that the gradient descent update rule for a given parameter $p$ and a learning rate $alpha$ isn,
    n,
    $$p gets p - alpha  frac{partial mathcal{E}}{partial p}$$
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    num_steps = 1000n,
    alpha = 1n,
    for step in range(num_steps)        n,
        grads, loss = backprop(data, labels, params)n,
        for k in paramsn,
            params[k] -= alpha  grads[k]n,
    n,
        # Print loss every so often.n,
        if step % 50 == 0n,
            print(Step {3d}  Loss {3.2f}.format(step, loss))
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Visualizing the predictions
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    num_points = 200n,
    x1s = np.linspace(-6.0, 6.0, num_points)n,
    x2s = np.linspace(-6.0, 6.0, num_points)n,
    n,
    points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])n,
    Y = forward(points, params).reshape(num_points, num_points)n,
    X1, X2 = np.meshgrid(x1s, x2s)n,
    n,
    plt.pcolormesh(X1, X2, Y, cmap=plt.cm.get_cmap('YlGn'))n,
    plt.colorbar()n,
    plt.scatter(data[num_data2, 0], data[num_data2, 1], color='red') n,
    plt.scatter(data[num_data2, 0], data[num_data2, 1], color='blue') 
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Looking forward Automatic differentiationn,
    n,
    You probably noticed that manually deriving the backpropagation equations isn,
    slow and error prone. It becomes even easier to make an error when implementingn,
    in code. Luckily, we almost never have to derive the backwards equations by hand.n,
    Instead, we make use of automatic differentation software packaged in librariesn,
    such as PyTorch to compute derivatives for us.
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    import torchn,
    import torch.nn as nnn,
    import torch.optim as optimn,
    n,
    class PyTorchModel(nn.Module)n,
        def __init__(self)n,
            super(PyTorchModel, self).__init__()n,
            self.layer1 = nn.Linear(2, 3)n,
            self.layer2 = nn.Linear(3, 1)n,
        def forward(self, X)n,
            h = torch.tanh(self.layer1(X))n,
            return self.layer2(h)n
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    data_tensor = torch.Tensor(data)n,
    labels_tensor = torch.Tensor(labels).float()n,
    labels_tensor = labels_tensor.reshape([500, 1]) # same shape as `y` belown,
    model = PyTorchModel()n,
    n,
    criterion = nn.BCEWithLogitsLoss()n,
    optimizer = optim.SGD(model.parameters(), lr=1)n,
    n,
    num_steps = 1000n,
    for step in range(num_steps)        n,
        y = model(data_tensor) n,
        loss = criterion(y, labels_tensor)n,
        loss.backward()n,
        optimizer.step()n,
        optimizer.zero_grad()n,
    n,
        # Print loss every so often.n,
        if step % 50 == 0n,
            print(Step {3d}  Loss {3.2f}.format(step, float(loss)))
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   outputs [],
   source [
    num_points = 200n,
    x1s = np.linspace(-6.0, 6.0, num_points)n,
    x2s = np.linspace(-6.0, 6.0, num_points)n,
    n,
    points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])n,
    Y = torch.sigmoid(model(torch.Tensor(points)))n,
    Y = Y.detach().numpy() # convert to numpyn,
    Y = Y.reshape(num_points, num_points)n,
    X1, X2 = np.meshgrid(x1s, x2s)n,
    n,
    plt.pcolormesh(X1, X2, Y, cmap=plt.cm.get_cmap('YlGn'))n,
    plt.colorbar()n,
    plt.scatter(data[num_data2, 0], data[num_data2, 1], color='red') n,
    plt.scatter(data[num_data2, 0], data[num_data2, 1], color='blue') 
   ]
  }
 ],
 metadata {},
 nbformat 4,
 nbformat_minor 2
}
